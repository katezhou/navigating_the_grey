{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfb9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65339ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/u/scr/katezhou/benchmarking/')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "989c8945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "{'gpt3': {'daily': Usage(period='2023-8-23', used=1000362, quota=None), 'monthly': Usage(period='2023-8', used=1000362, quota=None), 'total': Usage(period='all', used=6000057, quota=6000000)}, 'codex': {'daily': Usage(period=None, used=0, quota=10000)}, 'jurassic': {'daily': Usage(period=None, used=0, quota=10000)}, 'gooseai': {'daily': Usage(period=None, used=0, quota=10000)}, 'cohere': {'daily': Usage(period=None, used=0, quota=10000)}, 'text_to_image': {'daily': Usage(period=None, used=0, quota=10)}, 'dall_e': {'daily': Usage(period=None, used=0, quota=5)}, 'together_vision': {'daily': Usage(period=None, used=0, quota=30)}, 'gpt': {'daily': Usage(period=None, used=0, quota=None), 'monthly': Usage(period=None, used=0, quota=None)}, 'gpt4': {'daily': Usage(period='2023-8-23', used=74900, quota=None), 'monthly': Usage(period='2023-8', used=74900, quota=None), 'total': Usage(period='all', used=415714, quota=1000000)}}\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%run /u/scr/katezhou/benchmarking/utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887f169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():\n",
    "    \n",
    "    def __init__(self, name, sample_questions_files):\n",
    "        self.name = name\n",
    "        self.sample_questions = pd.DataFrame()\n",
    "\n",
    "        for sample_questions_file in sample_questions_files: \n",
    "            self.sample_questions = pd.concat([self.sample_questions, pd.read_csv(sample_questions_file)])\n",
    "         \n",
    "        if name == 'triviaqa':\n",
    "            self.sample_questions = self.sample_questions[['Question_example', 'Answer.MatchedWikiEntityName_example', 'Question', 'Answer.Aliases']]\n",
    "        elif name == 'countryqa':\n",
    "            self.sample_questions = self.sample_questions[['name_example', 'capital_example', 'name', 'capital']]\n",
    "        else:\n",
    "            self.sample_questions = self.sample_questions[['question_example', 'answer_example', 'question', 'answer']]\n",
    "            \n",
    "        self.sample_questions.columns = ['question_example', 'answer_example', 'question', 'answer']\n",
    "        \n",
    "        if name == 'boolqa':\n",
    "            self.sample_questions['answer'] =  self.sample_questions['answer'].astype(str)\n",
    "            self.sample_questions['answer_example'] =  self.sample_questions['answer_example'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac0ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_one_token_responses(df, answer_column_name):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    #tokenize all words using GPT2's tokenizer\n",
    "    df['tokens'] = df[answer_column_name].apply(lambda x: len(tokenizer(\" \" + str(x))['input_ids']))\n",
    "    df = df[df['tokens']==1]\n",
    "    #length of single token answer\n",
    "    print(\"Total number of one token answer examples:\", len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f6fae",
   "metadata": {},
   "source": [
    "# Querying and checking answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e535084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_processing(token):\n",
    "    token_prob_mapping = token.top_logprobs\n",
    "    token_prob_mapping = {k: v for k, v in sorted(token_prob_mapping.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    topk_predictions = list(token_prob_mapping.keys())\n",
    "    topk_predictions_log_prob = list(token_prob_mapping.values())\n",
    "\n",
    "    return topk_predictions, topk_predictions_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f710c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gold_answer(topk_predictions, topk_predictions_log_prob, answer, name):\n",
    "    #   if the actual answer is in the top 5 predictions\n",
    "    topk_predictions = [x.strip(\" \").lower() for x in topk_predictions]\n",
    "    gold_answer_logprob = 0\n",
    "\n",
    "    for index, prediction in enumerate(topk_predictions):\n",
    "        if (name == 'triviaqa'):\n",
    "            if (prediction in answer):\n",
    "                #Sum together all possible correct answers\n",
    "                gold_answer_logprob += np.e**(topk_predictions_log_prob[index])\n",
    "                print(\"one answer\", prediction, gold_answer_logprob)\n",
    "        else:\n",
    "            if (prediction == answer[0]):\n",
    "                #Sum together all possible correct answers\n",
    "                gold_answer_logprob += np.e**(topk_predictions_log_prob[index])\n",
    "                print(\"one answer\", prediction, gold_answer_logprob)\n",
    "\n",
    "    print(\"final prob\", gold_answer_logprob)\n",
    "    return gold_answer_logprob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e3302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_prompts(current_df, name, TOTAL_QUESTIONS_TO_QUERY):\n",
    "    prompts_to_use = pd.DataFrame()\n",
    "    #go through all the example prompts\n",
    "    for n, row in current_df.sample_questions[:TOTAL_QUESTIONS_TO_QUERY].iterrows():\n",
    "        \n",
    "        question = row['question']\n",
    "            \n",
    "        answer = [x.strip(\"\\'\") for x in row['answer'].lower().strip('][').split(', ')]\n",
    "        \n",
    "        if current_df.name == 'countryqa' or current_df.name == 'triviaqa':\n",
    "            prompt = \"Q: \" + question + \"\\nA:\" + name           \n",
    "        else:\n",
    "            prompt = \"Q: \" + question + \" A:\" + name\n",
    "\n",
    "        prompts_to_use = pd.concat([prompts_to_use, pd.DataFrame([[prompt, \n",
    "                                                                   question, \n",
    "                                                                   answer]])])\n",
    "    prompts_to_use.columns = ['prompt', \n",
    "                           'question', \n",
    "                           'answer']\n",
    "    return prompts_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f0ff8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def query_api(prompts_to_use, current_df, name, TOTAL_QUESTIONS_TO_QUERY, TEMPERATURE, MODEL_NAME, NUM_COMPLETIONS, STOP_SEQUENCES, ECHO_PROMPT, TOP_K_PER_TOKEN, MAX_TOKENS):\n",
    "    \n",
    "    for n, row in prompts_to_use[:TOTAL_QUESTIONS_TO_QUERY].iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        prompt = row['prompt']\n",
    "        answer = row['answer']\n",
    "\n",
    "        print(\"prompt\", prompt)\n",
    "        print(\"answer\", answer)\n",
    "        print(\"----\")\n",
    "        \n",
    "        request = Request(prompt=prompt, temperature=TEMPERATURE, model=MODEL_NAME, num_completions=NUM_COMPLETIONS, max_tokens=MAX_TOKENS, stop_sequences=STOP_SEQUENCES, echo_prompt=ECHO_PROMPT, top_k_per_token=TOP_K_PER_TOKEN)\n",
    "        request_result: RequestResult = service.make_request(auth, request)\n",
    "\n",
    "        print(\"hello\")\n",
    "        #one completion\n",
    "        sequence = request_result.completions[0]\n",
    "        text = [x.text for x in sequence.tokens]\n",
    "        print(text)\n",
    "\n",
    "        #make sure at least one token was generated\n",
    "        if len(sequence.tokens) == 0:\n",
    "            current_responses = pd.DataFrame([[prompt,\n",
    "                                               question, \n",
    "                                               answer,\n",
    "                                               -1,\n",
    "                                               -1,\n",
    "                                               \"\",\n",
    "                                               name] +\n",
    "                                               [] +\n",
    "                                               []])\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            topk_predictions_answer_list = [[]] * MAX_TOKENS\n",
    "            topk_predictions_log_prob_answer_list = [[]] * MAX_TOKENS\n",
    "\n",
    "            best_token_index = 0\n",
    "            best_token_value = -10000\n",
    "            for n, token in enumerate(sequence.tokens):\n",
    "                topk_predictions_answer, topk_predictions_log_prob_answer = token_processing(token)\n",
    "                topk_predictions_answer_list[n] = topk_predictions_answer\n",
    "                topk_predictions_log_prob_answer_list[n] = topk_predictions_log_prob_answer\n",
    "\n",
    "                gold_answer_logprob_answer = check_gold_answer(topk_predictions_answer, topk_predictions_log_prob_answer, answer, current_df.name)\n",
    "                if (gold_answer_logprob_answer > best_token_value):\n",
    "                    best_token_value = gold_answer_logprob_answer\n",
    "                    best_token_index = n\n",
    "\n",
    "            current_responses = pd.DataFrame([[prompt,\n",
    "                                               question, \n",
    "                                               answer,\n",
    "                                               best_token_value,\n",
    "                                               best_token_index,\n",
    "                                               text,\n",
    "                                               name] +\n",
    "                                               topk_predictions_answer_list +\n",
    "                                               topk_predictions_log_prob_answer_list])\n",
    "        \n",
    "        current_responses.to_csv(output_file, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c231fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(folder_path, dataset_name):\n",
    "    \n",
    "    if dataset_name == 'countryqa':\n",
    "        df = dataset('countryqa', [folder_path + \"countryqa/data/all_questions.csv\"])\n",
    "        df.sample_questions = filter_one_token_responses(df.sample_questions, 'answer')\n",
    "    else:\n",
    "        df = dataset(dataset_name, [folder_path + dataset_name + \"/data/sample_questions_single_token_answers_50.csv\",\n",
    "                                       folder_path + dataset_name + \"/data/sample_questions_single_token_answers_50_2.csv\",\n",
    "                                      folder_path +  dataset_name + \"/data/sample_questions_single_token_answers_100.csv\"])\n",
    "    print(\"number of questions read in\", len(df.sample_questions))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c721d",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ca5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results():\n",
    "    queried = pd.read_csv(output_file, header=None).drop_duplicates()\n",
    "    queried.columns = ['prompt', \n",
    "                       'question', \n",
    "                       'answer',\n",
    "                       'best_gold_answer_logprob_answer',\n",
    "                       'best_token_index',\n",
    "                       'text',\n",
    "                       'name'] + ['topk_predictions_answer_' + str(x) for x in range(0, 10)] + ['topk_predictions_log_prob_answer_' + str(x) for x in range(0, 10)]\n",
    "\n",
    "    print(\"output file\", output_file)\n",
    "\n",
    "    queried['name'] = queried['name'].fillna(\"unprompted\")\n",
    "    queried = queried[queried['name'] != \"It's\"]\n",
    "    # queried = queried.dropna(subset='topk_predictions_answer_0')\n",
    "    display(queried.groupby(\"name\").mean())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d349cccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(dataset_name, model_name):\n",
    "    folder_path = '/u/scr/katezhou/Uncertainty/interpretability/'\n",
    "\n",
    "    TOTAL_QUESTIONS_TO_QUERY = 200\n",
    "    NUM_COMPLETIONS = 1\n",
    "    MAX_TOKENS = 10\n",
    "    TEMPERATURE = 1\n",
    "    TOP_K_PER_TOKEN = 50\n",
    "    ECHO_PROMPT = False\n",
    "    STOP_SEQUENCES = []\n",
    "\n",
    "    prefix_df = pd.read_csv(folder_path + \"uncertainty_template_meta.csv\")\n",
    "    PREFIXES = [\"\"] + [\" \" + x for x in  prefix_df['name'].values]\n",
    "    PREFIXES.remove(\" unprompted\")\n",
    "    PREFIXES.remove( \" I don't think it's\")\n",
    "    PREFIXES.remove(\" I seriously doubt it's\")\n",
    "    print(\"number of prefixes\", len(PREFIXES))\n",
    "\n",
    "    current_df = get_datasets(folder_path, dataset_name)\n",
    "    MODEL_NAME = \"openai/\" + model_name\n",
    "    print(\"dataset\", dataset_name, \"model name\", model_name)\n",
    "\n",
    "    if (current_df.name == 'jeopardy'):\n",
    "        name = \"gricean_experiment_qa_cleaned\"\n",
    "        current_df.sample_questions['question_example'] = current_df.sample_questions['question_example'].apply(lambda x: x.replace(\"<br />\", \" \"))\n",
    "        current_df.sample_questions['question'] = current_df.sample_questions['question'].apply(lambda x: x.replace(\"<br />\", \" \"))\n",
    "    else:\n",
    "        name = \"gricean_experiment_qa\"\n",
    "\n",
    "    path = current_df.name + '/results/'\n",
    "    output_file = folder_path + path + str(MAX_TOKENS) + \"_\" + str(TEMPERATURE)[-1:] + \"_\" + MODEL_NAME.replace(\"/\", \"_\") + \"_\" + name + '.csv'\n",
    "\n",
    "    print(\"output_file\", output_file)\n",
    "\n",
    "    queried = pd.DataFrame()\n",
    "    queried_prompts = set()\n",
    "    if os.path.exists(output_file):\n",
    "\n",
    "        queried = pd.read_csv(output_file, header=None).drop_duplicates()\n",
    "        queried.columns = ['prompt',\n",
    "                           'question', \n",
    "                           'answer',\n",
    "                           'best_gold_answer_logprob_answer',\n",
    "                           'best_token_index',\n",
    "                           'text',\n",
    "                           'name'] + ['topk_predictions_answer_' + str(x) for x in range(0, 10)] + ['topk_predictions_log_prob_answer_' + str(x) for x in range(0, 10)]\n",
    "        queried_prompts = set(queried['prompt'].to_list())\n",
    "\n",
    "    total_prompts_to_query = 0\n",
    "    print()\n",
    "    for name in PREFIXES:\n",
    "        prompts_to_use = building_prompts(current_df, name, TOTAL_QUESTIONS_TO_QUERY)\n",
    "        prompts_set = set(prompts_to_use['prompt'].to_list())\n",
    "\n",
    "        print(\"template:\", name)\n",
    "        print('len prompts to use', len(prompts_set))\n",
    "        print('already queried', len(queried_prompts.intersection(prompts_set)))\n",
    "        prompts_set = prompts_set - queried_prompts\n",
    "        print('prompts to query', len(prompts_set))\n",
    "        total_prompts_to_query += len(prompts_set)\n",
    "    #     query_api(prompts_to_use[prompts_to_use['prompt'].isin(prompts_set)], current_df, name, TOTAL_QUESTIONS_TO_QUERY, TEMPERATURE, MODEL_NAME, NUM_COMPLETIONS, STOP_SEQUENCES, ECHO_PROMPT, TOP_K_PER_TOKEN, MAX_TOKENS)\n",
    "\n",
    "    print(\"Total prompts to query\", total_prompts_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4dc7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of prefixes 51\n",
      "number of questions read in 220\n",
      "dataset naturalqa model name ada\n",
      "output_file /u/scr/katezhou/Uncertainty/interpretability/naturalqa/results/10_1_openai_ada_gricean_experiment_qa.csv\n",
      "\n",
      "template: \n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  It could be\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  Considering all the options it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  It probably is\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  Maybe it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  Perhaps it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  I don't know maybe it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  I suppose it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  I would need to double check but maybe it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  I wouldn't put money on it but maybe it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  I'm not an expert but maybe it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  I think it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  I feel like it should be\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  It should be\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  To the best of my knowledge it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  As far as I'm aware it's\n",
      "len prompts to use 200\n",
      "already queried 200\n",
      "prompts to query 0\n",
      "template:  I vaguely remember it's\n",
      "len prompts to use 200\n",
      "already queried 195\n",
      "prompts to query 5\n",
      "template:  It's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  It must be\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  I'm certain it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Undoubtedly it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  We realize it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  We understand it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  We know it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  I am 100% sure it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  With 100% confidence it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Wikipedia suggests it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Online says it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  The internet says it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Wikipedia claims it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Wikipedia says it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  I read on the internet it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  I read on Wikipedia it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  I read online it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Apparently it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Presumably it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Rumor says it it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  I was told it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  I've heard it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  They told me it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Allegedly it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  The most recent evidence shows it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  The rules state it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Two recent studies demonstrate it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Wikipedia acknowledges it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Wikipedia confirms it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Our lab has shown it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  According to the latest research it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  We can see in the textbook that it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  It is known that it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "template:  Evidently it's\n",
      "len prompts to use 200\n",
      "already queried 0\n",
      "prompts to query 200\n",
      "Total prompts to query 6805\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     sys.argv[1:][0], sys.argv[1:][1]\n",
    "    main('naturalqa', 'ada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed92550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5ea42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
